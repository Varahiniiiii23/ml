# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Generate synthetic data
np.random.seed(42)
X = 2 - 3 * np.random.rand(100, 1)  # Feature
y = X**2 + np.random.randn(100, 1) * 0.5  # Target with some noise

# Split the dataset into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create Polynomial Features
degree = 2  # You can change this to test higher-degree polynomials
polynomial_features = PolynomialFeatures(degree=degree)
X_poly_train = polynomial_features.fit_transform(X_train)
X_poly_test = polynomial_features.transform(X_test)

# Create and train the Polynomial Regression model
poly_model = LinearRegression()
poly_model.fit(X_poly_train, y_train)

# Make predictions
y_pred_poly = poly_model.predict(X_poly_test)

# Calculate performance metrics
mse_poly = mean_squared_error(y_test, y_pred_poly)
r2_poly = r2_score(y_test, y_pred_poly)

# Visualize the results
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, color='red', label='Actual data')
plt.scatter(X_test, y_pred_poly, color='blue', label='Predicted data (Polynomial)', marker='x')
plt.title('Polynomial Regression (Degree = {})'.format(degree))
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

# Output performance metrics
print("Polynomial Regression Performance:")
print(f"Mean Squared Error: {mse_poly:.2f}")
print(f"R^2 Score: {r2_poly:.2f}")
