import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture

# Generate synthetic data: two Gaussian distributions
np.random.seed(42)
n_samples = 500
# First Gaussian
mean1 = [0, 0]
cov1 = [[1, 0.5], [0.5, 1]]
data1 = np.random.multivariate_normal(mean1, cov1, n_samples // 2)

# Second Gaussian
mean2 = [5, 5]
cov2 = [[1, -0.5], [-0.5, 1]]
data2 = np.random.multivariate_normal(mean2, cov2, n_samples // 2)

# Combine the data into a single dataset
data = np.vstack([data1, data2])

# Visualize the synthetic data
plt.scatter(data[:, 0], data[:, 1], alpha=0.5)
plt.title('Synthetic Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# Apply the Expectation-Maximization Algorithm using Gaussian Mixture Model
n_components = 2  # Number of Gaussian components
gmm = GaussianMixture(n_components=n_components, max_iter=100, random_state=42)
gmm.fit(data)

# Predict the cluster for each data point
labels = gmm.predict(data)

# Visualize the results
plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', alpha=0.5)
plt.title('GMM Clustering Results')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# Print the parameters of the Gaussian components
print("Means of the Gaussian components:")
print(gmm.means_)
print("\nCovariances of the Gaussian components:")
print(gmm.covariances_)
print("\nWeights of the Gaussian components:")
print(gmm.weights_)
